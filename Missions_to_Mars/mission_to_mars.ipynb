{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups up chromedriver and the browser control\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this can break from blackmagic, ask gretel\n",
    "# pulls the first index in list_text as variable recent\n",
    "recent = soup.find_all(\"div\", class_=\"list_text\")[0]\n",
    "# finds the title and paragraph text within recent\n",
    "news_title = recent.find(\"div\",class_=\"content_title\").text\n",
    "news_p = recent.find(\"div\",class_=\"article_teaser_body\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign url variable to next site\n",
    "url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locates .jpg link from feature image \n",
    "fancy_href = soup.find_all(\"a\", class_=\"button fancybox\")[0].get(\"data-fancybox-href\")\n",
    "# completes link url\n",
    "featured_image_url = 'https://www.jpl.nasa.gov' + fancy_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign url variable to next site\n",
    "url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "\n",
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "# this sleep step is longer to account for twitter restrictions\n",
    "browser.visit(url)\n",
    "time.sleep(30)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls all possible tweets\n",
    "tweets = soup.find_all(\"span\", class_=\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\")\n",
    "# sets up empty weather update tweet list\n",
    "update_tweets = []\n",
    "# filters possible tweets to just those that include marker signifying an update\n",
    "for tweet in tweets:\n",
    "    if \"InSight sol\" in tweet.text:\n",
    "        update_tweets.append(tweet.text)\n",
    "# assigns latest update to new variable\n",
    "mars_weather = update_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign url variable to next site\n",
    "url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = soup.find_all(\"table\", class_=\"tablepress tablepress-id-p-mars\")\n",
    "mars_df = pd.read_html(str(table))[0]\n",
    "mars_df = mars_df.set_index(0)\n",
    "mars_df = mars_df.rename(columns={1:\"value\"})\n",
    "mars_df = mars_df.rename_axis(None)\n",
    "mars_table = mars_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign url variable to next site\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls urls for enhanced hemisphere images\n",
    "results = soup.find_all(\"a\", class_=\"itemLink product-item\")\n",
    "for result in results:\n",
    "    if result.text == 'Cerberus Hemisphere Enhanced':\n",
    "        cerb_url = 'https://astrogeology.usgs.gov' + result.get(\"href\")\n",
    "    if result.text == 'Schiaparelli Hemisphere Enhanced':\n",
    "        schia_url = 'https://astrogeology.usgs.gov' + result.get(\"href\")\n",
    "    if result.text == 'Syrtis Major Hemisphere Enhanced':\n",
    "        syrtm_url = 'https://astrogeology.usgs.gov' + result.get(\"href\")\n",
    "    if result.text == 'Valles Marineris Hemisphere Enhanced':\n",
    "        vallm_url = 'https://astrogeology.usgs.gov' + result.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(cerb_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# collects url of full size image\n",
    "results = soup.find_all(\"a\")\n",
    "for result in results:\n",
    "    if result.text == 'Sample':\n",
    "        cerb_url = result.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(schia_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# collects url of full size image\n",
    "results = soup.find_all(\"a\")\n",
    "for result in results:\n",
    "    if result.text == 'Sample':\n",
    "        schia_url = result.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(syrtm_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# collects url of full size image\n",
    "results = soup.find_all(\"a\")\n",
    "for result in results:\n",
    "    if result.text == 'Sample':\n",
    "        syrtm_url = result.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets browser to visit the url and then wait to ensure all of the page loads\n",
    "browser.visit(vallm_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# builds soup object for data scraping\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# collects url of full size image\n",
    "results = soup.find_all(\"a\")\n",
    "for result in results:\n",
    "    if result.text == 'Sample':\n",
    "        vallm_url = result.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls = [\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": vallm_url},\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": cerb_url},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": schia_url},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": syrtm_url},\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
